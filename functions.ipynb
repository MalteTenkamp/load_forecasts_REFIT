{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def scatter_plot(DataFrame):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(DataFrame.index, DataFrame['Aggregate'], s=.5) # 's=10' makes the dots smaller\n",
    "    plt.xlabel('Time') # Set the x-axis label to 'Time'\n",
    "    plt.ylabel('Aggregate') # Set the y-axis label to 'Aggregate'\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def big_plot(dataframe):\n",
    "    dataframe.plot(linewidth=1, figsize=(20,20))\n",
    "\n",
    "\n",
    "def read_clean_house(number_of_house, number_of_rows, first_rows=True, drop_appliances=False):\n",
    "    filename = f'CLEAN_REFIT_081116/CLEAN_House{number_of_house}.csv'\n",
    "    \n",
    "    if first_rows:\n",
    "        house = pd.read_csv(filename, nrows=number_of_rows)\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            total_rows = sum(1 for line in f)        \n",
    "            skiprows = total_rows - number_of_rows\n",
    "            house = pd.read_csv(filename, skiprows=range(1, skiprows))\n",
    "    house['Time'] = pd.to_datetime(house['Time'])\n",
    "    house.set_index('Time', inplace=True)\n",
    "    house.drop(columns=['Unix', 'Issues'], inplace=True)\n",
    "    if drop_appliances:\n",
    "        appliance_columns = ['Appliance1', 'Appliance2', 'Appliance3', 'Appliance4', 'Appliance5', 'Appliance6', 'Appliance7', 'Appliance8', 'Appliance9']\n",
    "        house.drop(columns=[col for col in appliance_columns if col in house.columns], inplace=True)\n",
    "    peaks_cut = house[house['Aggregate'] < 10500]\n",
    "    return peaks_cut\n",
    "\n",
    "\n",
    "def process_sequences(dataframe):\n",
    "    sequences = []\n",
    "    current_sequence = {'start_time': None, 'end_time': None, 'values': []}\n",
    "    prev_idx = None  # Keep track of the previous row's index\n",
    "\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        value = row['Aggregate']\n",
    "        \n",
    "        # Start a new sequence if it's the first value or the value changes significantly\n",
    "        if not current_sequence['values'] or abs(value - current_sequence['values'][-1]) / current_sequence['values'][-1] > 0.05:\n",
    "            if current_sequence['values']:\n",
    "                # Finalize the current sequence using the previous row's timestamp as the end time\n",
    "                current_sequence['end_time'] = prev_idx\n",
    "                sequences.append(current_sequence)\n",
    "                # Start a new sequence\n",
    "                current_sequence = {'start_time': idx, 'end_time': None, 'values': [value]}\n",
    "            else:\n",
    "                # This is the first value in the sequence\n",
    "                current_sequence['start_time'] = idx\n",
    "                current_sequence['values'].append(value)\n",
    "        else:\n",
    "            # Continue the current sequence\n",
    "            current_sequence['values'].append(value)\n",
    "        \n",
    "        prev_idx = idx  # Update the previous index at the end of the loop\n",
    "\n",
    "    # Finalize the last sequence using the last known timestamp as the end time\n",
    "    if current_sequence['values']:\n",
    "        current_sequence['end_time'] = prev_idx\n",
    "        sequences.append(current_sequence)\n",
    "\n",
    "    start_time_data = {\n",
    "        'Time': [seq['start_time'] for seq in sequences],\n",
    "        'Aggregate': [sum(seq['values']) / len(seq['values']) for seq in sequences]\n",
    "    }\n",
    "    end_time_data = {\n",
    "        'Time': [seq['end_time'] for seq in sequences],\n",
    "        'Aggregate': [sum(seq['values']) / len(seq['values']) for seq in sequences]\n",
    "    }\n",
    "\n",
    "    start_time_data_df = pd.DataFrame(start_time_data)\n",
    "    end_time_data_df = pd.DataFrame(end_time_data)\n",
    "\n",
    "    result_df = pd.concat([start_time_data_df, end_time_data_df], ignore_index=True)\n",
    "    concatenated_result_df = result_df.sort_values(by='Time').set_index('Time')\n",
    "\n",
    "\n",
    "    return concatenated_result_df[~concatenated_result_df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "def correlation_between_original_and_processed(original, processed):\n",
    "    original_data = original['2013-11-29 06:00:00':'2013-11-29 9:00:00']\n",
    "    processed_data = processed\n",
    "    expanded_processed_data = processed_data.reindex(original_data.index, method='ffill')\n",
    "    correlation = original_data['Aggregate'].corr(expanded_processed_data['Aggregate'])\n",
    "\n",
    "    print(f\"Correlation coefficient: {correlation}\")\n",
    "    return (f\"Correlation coefficient: {correlation}\")\n",
    "\n",
    "\n",
    "def cumulate_consumption(dataframe):\n",
    "    end_times = dataframe.index.to_series().shift(-1) - pd.Timedelta(seconds=1)\n",
    "\n",
    "    time_diffs_seconds = (end_times - dataframe.index.to_series()).dt.total_seconds().fillna(method='ffill')\n",
    "\n",
    "    time_diffs_hours = time_diffs_seconds / 3600\n",
    "\n",
    "    dataframe['Energy'] = dataframe['Aggregate'] * time_diffs_hours\n",
    "\n",
    "    dataframe['Cumulative Energy'] = dataframe['Energy'].cumsum()\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def thirty_min_steps_and_explicit_time_features(house:pd.DataFrame):\n",
    "    resampled = house.resample('30T').mean()\n",
    "    resampled['hour'] = resampled.index.hour\n",
    "    resampled['day_of_week'] = resampled.index.dayofweek  # Monday=0, Sunday=6\n",
    "    resampled['month'] = resampled.index.month\n",
    "    resampled['is_weekend'] = resampled['day_of_week'].isin([5, 6]).astype(int)\n",
    "    resampled.dropna(inplace=True)\n",
    "    return resampled\n",
    "\n",
    "def standardizing_and_sequencing(resampled_and_feature_engineered_house:pd.DataFrame, scaler=StandardScaler()):\n",
    "    resampled_and_feature_engineered_house['Aggregate_standardized'] = scaler.fit_transform(resampled_and_feature_engineered_house[['Aggregate']])\n",
    "    resampled_and_feature_engineered_house.drop(columns=['Aggregate'], inplace=True)\n",
    "    for i in range(1, 7):\n",
    "        resampled_and_feature_engineered_house[f'Aggregate_t+{i}'] = resampled_and_feature_engineered_house['Aggregate_standardized'].shift(-i)\n",
    "    \n",
    "    sequence_length = 20  # Assuming each record is 30 minutes, so 20 records for 10 hours\n",
    "    num_sequences = 3000  # Desired number of sequences\n",
    "\n",
    "    # Generate random start points based on index positions, not datetime values\n",
    "    max_start_pos = len(resampled_and_feature_engineered_house) - sequence_length\n",
    "    start_pos = random.sample(range(max_start_pos), k=min(num_sequences, max_start_pos))\n",
    "\n",
    "    sequences = []\n",
    "    for pos in start_pos:\n",
    "        # Extract sequences by index positions\n",
    "        sequence = resampled_and_feature_engineered_house.iloc[pos:pos + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return scaler, sequences\n",
    "\n",
    "\n",
    "def print_sequences(sequences, start=0 , end = 10):\n",
    "    for i, sequence in enumerate(sequences[start:end]):\n",
    "        print(f\"Sequence with index {i}, being sequence no. {i+1}:\")\n",
    "        print(sequence)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "def features_list_and_labels_list (sequences, aggregate_columns = ['Aggregate_t+1', 'Aggregate_t+2', 'Aggregate_t+3', 'Aggregate_t+4', 'Aggregate_t+5', 'Aggregate_t+6']):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    for sequence in sequences:\n",
    "        features = sequence.drop(columns=aggregate_columns) #'features' is a pd.DataFrame\n",
    "        features_list.append(features)\n",
    "        labels = sequence.iloc[-1][aggregate_columns] #'labels' is a pd.Series\n",
    "        labels_list.append(labels)\n",
    "    return features_list, labels_list\n",
    "\n",
    "def split_and_to_numpy(features_list, labels_list, train=.7, test=.15):\n",
    "    if not len(features_list) == len(labels_list):\n",
    "        print('feature liste und label liste nicht gleich lang')\n",
    "        return\n",
    "    else:\n",
    "        size = len(features_list)\n",
    "        train_end = int(train*size)\n",
    "        test_end = int(train_end + size*test)\n",
    "\n",
    "        # Split the features\n",
    "        X_train = features_list[:train_end]\n",
    "        X_test = features_list[train_end:test_end]\n",
    "        X_validate = features_list[test_end:]\n",
    "\n",
    "        # Split the labels\n",
    "        y_train = labels_list[:train_end]\n",
    "        y_test = labels_list[train_end:test_end]\n",
    "        y_validate = labels_list[test_end:]\n",
    "\n",
    "        # Convert features into 3D array\n",
    "        X_train_arr = np.array([df.to_numpy() for df in X_train])\n",
    "        X_test_arr = np.array([df.to_numpy() for df in X_test])\n",
    "        X_validate_arr = np.array([df.to_numpy() for df in X_validate])\n",
    "\n",
    "        # Convert labels into 2D array\n",
    "        y_train_arr = np.array([series.to_numpy() for series in y_train])\n",
    "        y_test_arr = np.array([series.to_numpy() for series in y_test])\n",
    "        y_validate_arr = np.array([series.to_numpy() for series in y_validate])\n",
    "\n",
    "        return X_train_arr, X_test_arr, X_validate_arr, y_train_arr, y_test_arr, y_validate_arr\n",
    "\n",
    "\n",
    "def forward_fill_not_first_NaNs (arr:np.array):\n",
    "    mask = np.isnan(arr)\n",
    "    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n",
    "    np.maximum.accumulate(idx,axis=1, out=idx)\n",
    "    arr = arr[np.arange(idx.shape[0])[:,None], idx]\n",
    "    return arr\n",
    "\n",
    "\n",
    "def encode_col_cyclical(data:pd.DataFrame, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    data = data.drop(columns=col)\n",
    "    return data\n",
    "\n",
    "def encode_cyclical(house:pd.DataFrame):\n",
    "    house = encode_col_cyclical(house, 'hour', 24)\n",
    "    house = encode_col_cyclical(house, 'day_of_week', 7)\n",
    "    house = encode_col_cyclical(house, 'month', 12)\n",
    "    return house\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
